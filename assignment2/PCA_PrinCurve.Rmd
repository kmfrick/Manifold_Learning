---
title: 'Assignment: PCA and principal curves'
author: "Kevin Frick, Patrick Lutz, and Sonia Petrini"
date: "28 Nov 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment: PCA and principal curves

## Part1: PCA

Reading the data:
```{r}
library(dplyr)
zip.train <- read.table("zip.train")
zeros <- filter(zip.train, V1==0)
zeros <- as.matrix(zeros[,-1])
```

a. Do a hierarchical clustering of these data using the ward.D method, plot the resulting dendogram and cut it into k=4 clusters
```{r}
dendo <- hclust(dist(zeros), method="ward.D")
plot(dendo)
dendo.4 <- cutree(dendo,4)
```

b. Plot the average digit at each cluster.
```{r}
num_clusters <- 4
plot.zip <- function(x,use.first=TRUE, main=NULL){
  x<-as.numeric(x)
  if (use.first){
    x.mat <- matrix(x,16,16)
  }else{
    x.mat <- matrix(x[-1],16,16)
  }
  image(1:16,1:16,x.mat[,16:1],
        col=gray(seq(1,0,l=12)))
  if (!is.null(main)) title(main)
  else if (!use.first) title(x[1])

  #col=gray(seq(1,0,l=2)))
}
for (i in 1:num_clusters) {
  plot.zip(colMeans(zeros[which(dendo.4==i),]), main = paste("Cluster", i))
}

```

c. Compute the principal components for this data set. Plot the scatterplot of the scores in the first two PCs, using a different color for points in different clusters.
```{r}
options(error = function() traceback(10))

zip_pc <- princomp(zeros)
for (i in 1:5) {
  pts_in_cluster <- which(dendo.4==i)
  pc_scores <- zip_pc$scores[pts_in_cluster,1:2]
  xmin <- min(zip_pc$scores[,1])
  xmax <- max(zip_pc$scores[,1])
  ymin <- min(zip_pc$scores[,2])
  ymax <- max(zip_pc$scores[,2])

  if (i == 1) plot(pc_scores, col=i, xlim=c(xmin, xmax), ylim=c(ymin,ymax))
  else points(pc_scores, col=i)
}
```

d. For each one of the k clusters obtained above, do the following tasks: (A unique scatter plot of the scores in PC1 and PC2 should be done, over which the k densities are represented imultaneously)
 - Consider the bivariate data set of the scores in PC1 and PC2 of the points in this cluster.
 - Estimate non-parametrically the joint density of (PC1,PC2), conditional to this cluster. Use the default bandwith values.
 - Represent the estimated bivariate density using the level curve that covers the 75% of the points in this cluster.
```{r error=FALSE}
library(sm)
options(error = function() traceback(10))

draw_density_plot <- function() {
  for (i in 1:num_clusters) {
  pts_in_cluster <- which(dendo.4==i)
  pc_scores <- zip_pc$scores[pts_in_cluster,1:2]

  if (i == 1) {
    xmin <- min(zip_pc$scores[,1])
    xmax <- max(zip_pc$scores[,1])
    ymin <- min(zip_pc$scores[,2])
    ymax <- max(zip_pc$scores[,2])
    plot(pc_scores, col=i, xlim=c(xmin, xmax), ylim=c(ymin,ymax))
  }
  else points(pc_scores, col=i)
  sm.density(pc_scores, col=i, display="slice", add=TRUE, props=c(75))
  }
}
draw_density_plot()

```

e. Over the previous plot, represent the principal curve obtained from the 256-dimensional set of zeros using the package princurve.
```{r}
library(princurve)
options(error = function() traceback(10))

for (df in seq(3, 9, 3)) {
  pc <- principal_curve(as.matrix(zeros), df=df)
  points_to_pc <- predict(zip_pc, pc$s)
  draw_density_plot()
  lines(points_to_pc[pc$ord,1:2],col=6,lwd=2)
  title(main=paste("Principal curve with df =", df))
}

```

We plot the principal curve for different values of the `df` parameter. We find that `df=6` allows the curve to smoothly pass through each cluster without being swayed by dense clusters (which starts to happen with `df = 9`) or being excessively noisy.

f. For each one of the k clusters obtained above, do the following tasks: (A unique plot should be done, at which the k densities are represented simultaneously)
 - Consider the univariate data set of the lambda scores over the principal curve of the points in this cluster.
 - Estimate non-parametrically the density function of lambda, conditional to this cluster. Use the default bandwith value.
 - Plot the estimated density function.

 We compute lambdas for the principal curve with `df = 6`.
```{r}
options(error = function() traceback(10))
pc <- principal_curve(as.matrix(zeros), df=6)
# Get appropriate values that allow for showing the entire density functions
xmin <- min(pc$lambda) - 10
xmax <- max(pc$lambda) + 10
# These are problematic to be found a priori, but a nice heuristic is max(p) = 1/10. This is then refined.
ymin <- 0
ymax <- 0.13

options(error = function() traceback(10))

for (i in 1:num_clusters) {
  pts_in_cluster <- which(dendo.4==i)

  if (i == 1) {
    sm.density(pc$lambda[pts_in_cluster], col=i, sm=75, xlab="Lambda", xlim=c(xmin, xmax), ylim=c(ymin, ymax))
  } else {
    sm.density(pc$lambda[pts_in_cluster], col=i, add=TRUE, sm=75)

  }
}
```



## Part2: Principal Curves

Get the data
```{r}
t <- seq(-1.5*pi,1.5*pi,l=100)
R<- 1
n<-75
sd.eps <- .15
set.seed(1)
y <- R*sign(t) - R*sign(t)*cos(t/R)
x <- -R*sin(t/R)
z <- (y/(2*R))^2
rt <- sort(runif(n)*3*pi - 1.5*pi)
eps <- rnorm(n)*sd.eps
ry <- R*sign(rt) - (R+eps)*sign(rt)*cos(rt/R)
rx <- -(R+eps)*sin(rt/R)
rz <- (ry/(2*R))^2 + runif(n,min=-2*sd.eps,max=2*sd.eps)
XYZ <- cbind(rx,ry,rz)
require(plot3D)
lines3D(x,y,z,colvar = NULL,
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=4,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```


a. Choose the value of the degrees of freedom df by leave-one-out cross-validation. Restrict the search of df to seq(2,8,by=1).

```{r}
sample <- sample.int(n = nrow(XYZ), size = floor(.75*nrow(XYZ)), replace = F)
train <- XYZ[sample, ]
test  <- XYZ[-sample, ]

# LOOCV to find best df
require(princurve)
dfs <- seq(2,8,by=1)
n_points <- nrow(XYZ)
distances <- c()

for (df in dfs){
  dist <- c()
  for(i in 1:n_points){
      validation <- t(as.matrix(XYZ[i,]))
      training <- XYZ[-i,]
      fit <- principal_curve(training,df=df)
      proj <- project_to_curve(validation,fit$s)
      dist[i] <- proj$dist
  }
  distances <- append(distances,mean(dist))
}


# plot of LOOCV results
df_opt_dist <- min(distances)
df_opt <- dfs[which(distances==df_opt_dist)]
plot(x=dfs,y=distances,ylab="Total Squared Porojecting Distance",xlab="Degrees of Freedom",main="")
abline(h=df_opt_dist,lty="dashed",col="red")
```


b. Give a graphical representation of the principal curve output for the optimal df and comment on the obtained results.

The best value of Degrees of Freedom according to Leave-One-Out CV is 6. As we can see from the figure below, this value allows the Principal Curve to cross the data points while preserving its smoothness: it allows for some bias, but it gets complex enough to grasp the shape of the data. The Principal Curve created with df = 6 does a much better job at approximating the data distribution compared to the lower df value considered, namely 2. Indeed, the model built with this last value clearly leads to underfitting, and a very high total squared projection distance.

Additionally, while drawing a set of three-dimensional line segments would result in an excessively busy plot, projecting the points into a two-dimensional space allows for using the `whiskers` function to see the projections of the points on a principal curve, sacrificing exactness for the sake of a qualitative evaluation.

```{r}
par(mfrow=c(1,2))

# Principal Curve with lowest df
fit_low <- principal_curve(XYZ,df=dfs[1])
# Principal Curve with best df
fit_opt <- principal_curve(XYZ,df=df_opt)

# Low df
princ_curve <- fit_low$s
# 3D
lines3D(princ_curve[,1],princ_curve[,2],princ_curve[,3],colvar = NULL,
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=3,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
title(main="Principal Curve with df=2")
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)

# Opt df
princ_curve <- fit_opt$s
# 3D
lines3D(princ_curve[,1],princ_curve[,2],princ_curve[,3],colvar = NULL,
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=3,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
title(main=paste("Principal Curve with df=", df_opt))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)

# 2D low
plot(fit_low,xlim=range(XYZ[,1]),ylim=range(XYZ[,2]),asp=1,
     main="Principal Curve with df=2",cex.main=1)
points(XYZ,col=4)
lines(fit_low)
points(fit_low)
whiskers(XYZ, fit_low$s)
# 2D opt
plot(fit_opt,xlim=range(XYZ[,1]),ylim=range(XYZ[,2]),asp=1,
     main=paste("Principal Curve with optimal df =",df_opt),cex.main=1)
points(XYZ,col=4)
lines(fit_opt)
points(fit_opt)
whiskers(XYZ, fit_opt$s)

```




c. Compute the leave-one-out cross-validation for df=50 and compare it with the result corresponding to the optimal df value you found before.

By performing LOOCV with df = 50, we find a total squared distance equal to 0.0394, which increases by approximately 60 % the performance of the model, compared to the value found with 6, the optimal Degrees of Freedom.

```{r}
# LOOCV for df = 50
df <- 50
df50_distances <- c()
for(i in 1:n_points){
      validation <- t(as.matrix(XYZ[i,]))
      training <- XYZ[-i,]
      fit <- principal_curve(training,df=df)
      proj <- project_to_curve(validation,fit$s)
      df50_distances[i] <- proj$dist
}
df50_dist <- mean(df50_distances)
print(paste("LOOCV score for df = 50:", df50_dist))
print(paste("LOOCV score for df =",df_opt,":",df_opt_dist))
change <- round(1- df50_dist/df_opt_dist,3)
print(paste("Change in performance with new df = 50:",change*100, "%"))
```

  - Before fitting the principal curve with df=50 and based only on the leave-one-out cross-validation values, what value for df do you think that is better, the previous optimal one or df=50?

  Setting df = 50 allows one to reduce the projection distance of the points from the curve by more than 60 %. However, this is expected, as increasing this parameter results in a tighter fit of the model, and thus to a decrease in error. As already mentioned, the optimal df value seems to be a very good option to balance the model's bias and variability, as it is quite complex but allows for some error. Given this consideration, largely increasing the Degrees of Freedom will certainly lead to overfit the data, building a Principal Curve that tries to traverse all the points, but will not be able to generalize.

- Fit now the principal curve with df=50 and plot the fitted curve in the 3D scatterplot of the original points.

```{r}
fit <- principal_curve(XYZ,df=50)
princ_curve <- fit$s
lines3D(princ_curve[,1],princ_curve[,2],princ_curve[,3],colvar = NULL,
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=3,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```


- Now, what value of df do you prefer?

From the visualization of the Principal Curve built with df = 50, we can confirm our expectation: this high df value leads to overfitting the data, creating a very complex and not smooth curve.

- The overfitting with df=50 is clear. Nevertheless leave-one-out cross-validation has not been able to
detect this fact. Why do you think that df=50 is given a so good value of leave-one-out cross-validation?

LOOCV is an unbiased measure that only takes into account the model's ability to give accurate predictions, but it does not stricly measure overfitting. In order to assess the level of overfitting, a measure considering both the model's error and complexity should be used, such as the AIC in regression. Train/Test splits or k-fold cross-validation would not be adequate either, since they are simply biased estimates of LOOCV.

One method for model selection that could be used is the Local Continuity Meta-Criterion, which measures the degree of overlap between k-nearest-neighbor sets computed using the distance matrix of the points projected on the curve and the points in their original space. However, this criteron does not take into account model complexity either.

```{r}
LCMC <- function(D1,D2,Kp){
  D1 <- as.matrix(D1)
  D2 <- as.matrix(D2)
  n <- dim(D1)[1]
  N.Kp.i <- numeric(n)
  for (i in 1:n){
    N1.i <- sort.int(D1[i,],index.return = TRUE)$ix[1:Kp]
    N2.i <- sort.int(D2[i,],index.return = TRUE)$ix[1:Kp]
    N.Kp.i[i] <- length(intersect(N1.i, N2.i))
  }
  N.Kp<-mean(N.Kp.i)
  M.Kp.adj <- N.Kp/Kp - Kp/(n-1)

  return(list(N.Kp.i=N.Kp.i, M.Kp.adj=M.Kp.adj))
}

possible_df <- c(2:8, 50)

lcmc <- c()
for (df in possible_df) {
  fit <- principal_curve(XYZ,df=df)
  proj <- project_to_curve(XYZ, fit$s)
  proj <- as.matrix(proj$s)
  D1 <- dist(proj) # distance matrix among points projected onto the PC
  D2 <- dist(XYZ) # distance matrix among points
  lcmc_k <- c()
  for (k in 2:10) {
    lcmc_k <- append(lcmc_k, LCMC(D1, D2, k)$M.Kp.adj)
  }
  lcmc <- append(lcmc, mean(lcmc_k))
}

library(plotrix)
par(bty="n") # delete the box for a better look after the axis cut
gap.plot(possible_df, lcmc, gap = c(9, 48), gap.axis = "x", xlab = 'DoF',
          ylab = 'LCMC', xtics=possible_df)
axis(2)
abline(v=seq(8.99,9.2,.005), col="white")  # hiding vertical lines
axis.break(1,9,style="slash")

cat("M.Kp.adj(", possible_df[length(lcmc)],
    ") / M.Kp.adj(", possible_df[length(lcmc)-3],
    ") = ", lcmc[length(lcmc)]/lcmc[length(lcmc)-3])
```

Even using LCMC, we find that even this criterion is unable to give a worse score to a model with 50 DoF and one with 6 DoF, although the difference is lower than what we get with LOOCV (13%  improvement in favor of 50 DoF, vs 61% with LOOCV).

We believe this result is to be expected since even LCMC doesn't take into account model complexity, like AIC does.




